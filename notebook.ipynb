{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Sentiment Analysis with News Headlines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More and more hedge funds and independent traders are utilizing data science to process the wealth of information available from news headlines in the quest for profit. In this project, I will generate investment insights by applying sentiment analysis on financial news headlines, webscraped from FINVIZ.com. Repsecting data science ethics with regard to webscraping, I have downloaded various HTML files for two large firms: Facebook & Tesla.\n",
    "\n",
    "Through conducting sentiment analysis, we can examine the emotion behind the headlines and predict whether the market feels good or bad about a stock. Then, we can make educated guesses on how certain stocks will perform and trade accordingly. Below, we import these files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "html_tables = {}\n",
    "\n",
    "# for every data set in os dataset folder\n",
    "for table_name in os.listdir('datasets'):\n",
    "    # filepath\n",
    "    table_path = f'datasets/{table_name}'\n",
    "\n",
    "    # open as read-only, read into 'html'\n",
    "    table_file = open(table_path, 'r')\n",
    "    html = BeautifulSoup(table_file)\n",
    "\n",
    "    # add news-table to 'html_tables' dictionary\n",
    "    html_table = html.find(id = 'news-table')\n",
    "    html_tables[table_name] = html_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We've now obtained the table which contains all the headlines from each stock's HTML file. Before we move any further, we must investigate the structure in the data table. Let's read a single day of headlines for Tesla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla = html_tables['tsla_22sep.html']\n",
    "\n",
    "# store all table rows with <tr> tags\n",
    "tsla_tr = tsla.findAll('tr')\n",
    "\n",
    "for i, table_row in enumerate(tsla_tr) :\n",
    "\n",
    "    # store <a> elements in link_text\n",
    "    link_text = table_row.a.get_text()\n",
    "    # store <td> elements in data_text\n",
    "    data_text = table_row.a.get_text()\n",
    "\n",
    "    # print file count & text variables\n",
    "    print(f'File number {i+1}:'); print(link_text); print(data_text)\n",
    "\n",
    "    # exits loop after 4 rows\n",
    "    if i == 3: break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to parse the data for <strong>all</strong> tables into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed news list of lists\n",
    "parsed_news = []\n",
    "# Iterate through news, nested iterate through all tr tags in each \"news_table\"\n",
    "for file_name, news_table in html_tables.items():\n",
    "    for x in news_table.findAll('tr'):\n",
    "        # store read text in 'text'\n",
    "        text = x.get_text() \n",
    "        # scrape the text, split into a list\n",
    "        date_scrape = x.td.text.split()\n",
    "        \n",
    "        # if date_scrape only has 1 element, only load 'time'\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "        # otherwise, load both 'date' and 'time'\n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "\n",
    "        # extract stock ticker \n",
    "        stock_ticker = file_name.split(\"_\")[0]\n",
    "\n",
    "        # append all our information: ticker, date, time, headline\n",
    "        parsed_news.append([stock_ticker, date, time, x.a.text])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis is very sensitive to context. For example, saying \"This is so addicting.\" can be a positive statement when describing an exciting thing, like a video game, but can also be negative when we're talking about drugs. Like most professionals, financial journalists have their own writing style, so to extract sentiment from their headlines, must make NLTK think like a financial journalist. Let's add some keywords and sentiment values to our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK VADER for sentiment analysis\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# words and values\n",
    "keywords = {\n",
    "    'crushes': 10,\n",
    "    'beats': 5,\n",
    "    'misses': -5,\n",
    "    'trouble': -10,\n",
    "    'falls': -100,\n",
    "}\n",
    "# instantiate sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "# update the lexicon\n",
    "vader.lexicon.update(keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the data and the sentiment algorithm loaded, our next step is to programmatically predict sentiment out of news headlines. VADER is very high level; we do not require to adjust the model any further. We can proceed by converting our 'parsed_news' list of lists into a pandas DataFrame, and joining it with another DataFrame, containing the polarity scores for all headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['ticker', 'date', 'time', 'headline']\n",
    "\n",
    "# convert parsed_news into pandas DataFrame\n",
    "scored_news_df = pd.DataFrame(parsed_news, columns=columns)\n",
    "\n",
    "# get polarity scores from each headline\n",
    "scores = [vader.polarity_scores(headline) for headline in scored_news_df.headline]\n",
    "\n",
    "# convert scores into DataFrame\n",
    "scores_df = pd.DataFrame(scores)\n",
    "scored_news_df.columns = columns\n",
    "\n",
    "# join scored_news_df and scores_df\n",
    "scored_news_df = scored_news_df.join(scores_df)\n",
    "\n",
    "# convert date column from string to date\n",
    "scored_news_df['date'] = pd.to_datetime(scored_news_df.date).dt.date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d9defa72c2715dab9f7f172572cd30a1ab1a2083462d32ef96aadb7c6e0c73b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
