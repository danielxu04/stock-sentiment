{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Sentiment Analysis with News Headlines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More and more hedge funds and independent traders are utilizing data science to process the wealth of information available from news headlines in the quest for profit. In this project, I will generate investment insights by applying sentiment analysis on financial news headlines, webscraped from FINVIZ.com. Repsecting data science ethics with regard to webscraping, I have downloaded various HTML files for two large firms: Facebook & Tesla.\n",
    "\n",
    "Through conducting sentiment analysis, we can examine the emotion behind the headlines and predict whether the market feels good or bad about a stock. Then, we can make educated guesses on how certain stocks will perform and trade accordingly. Below, we import these files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "html_tables = {}\n",
    "\n",
    "# for every data set in os dataset folder\n",
    "for table_name in os.listdir('datasets'):\n",
    "    # filepath\n",
    "    table_path = f'datasets/{table_name}'\n",
    "\n",
    "    # open as read-only, read into 'html'\n",
    "    table_file = open(table_path, 'r')\n",
    "    html = BeautifulSoup(table_file)\n",
    "\n",
    "    # add news-table to 'html_tables' dictionary\n",
    "    html_table = html.find(id = 'news-table')\n",
    "    html_tables[table_name] = html_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We've now obtained the table which contains all the headlines from each stock's HTML file. Before we move any further, we must investigate the structure in the data table. Let's read a single day of headlines for Tesla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla = html_tables['tsla_22sep.html']\n",
    "\n",
    "# store all table rows with <tr> tags\n",
    "tsla_tr = tsla.findAll('tr')\n",
    "\n",
    "for i, table_row in enumerate(tsla_tr) :\n",
    "\n",
    "    # store <a> elements in link_text\n",
    "    link_text = table_row.a.get_text()\n",
    "    # store <td> elements in data_text\n",
    "    data_text = table_row.a.get_text()\n",
    "\n",
    "    # print file count & text variables\n",
    "    print(f'File number {i+1}:'); print(link_text); print(data_text)\n",
    "\n",
    "    # exits loop after 4 rows\n",
    "    if i == 3: break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to parse the data for <strong>all</strong> tables into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed news list\n",
    "parsed_news = []\n",
    "# Iterate through news, nested iterate through all tr tags in each \"news_table\"\n",
    "for file_name, news_table in html_tables.items():\n",
    "    for x in news_table.findAll('tr'):\n",
    "        # store read text in 'text'\n",
    "        text = x.get_text() \n",
    "        # scrape the text, split into a list\n",
    "        date_scrape = x.td.text.split()\n",
    "        \n",
    "        # if date_scrape only has 1 element, only load 'time'\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "        # otherwise, load both 'date' and 'time'\n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "\n",
    "        # extract stock ticker \n",
    "        stock_ticker = file_name.split(\"_\")[0]\n",
    "\n",
    "        # append all our information: ticker, date, time, headline\n",
    "        parsed_news.append([stock_ticker, date, time, x.a.text])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis is very sensitive to context. For example, saying \"This is so addicting.\" can be a positive statement when describing an exciting thing, like a video game, but can also be negative when we're talking about drugs. Like most professionals, financial journalists have their own writing style, so to extract sentiment from their headlines, must make NLTK think like a financial journalist. Let's add some keywords and sentiment values to our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK VADER for sentiment analysis\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# words and values\n",
    "keywords = {\n",
    "    'crushes': 10,\n",
    "    'beats': 5,\n",
    "    'misses': -5,\n",
    "    'trouble': -10,\n",
    "    'falls': -100,\n",
    "}\n",
    "# instantiate sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "# update the lexicon\n",
    "vader.lexicon.update(keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d9defa72c2715dab9f7f172572cd30a1ab1a2083462d32ef96aadb7c6e0c73b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
